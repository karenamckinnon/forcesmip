{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e54088",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ForceSMIP Example: LFCA (without training)\n",
    "\n",
    "The goal of this notebook is to demonstrate the use of low-frequency component analysis (LFCA) to estimate the forced and unforced components of climate change for a single variable, with surface air temperature used as an example, as in Wills et al. (2020, https://doi.org/10.1175/JCLI-D-19-0855.1). The basic idea of LFCA is to find the patterns that evolve the slowest within a dataset, a subset of which will be the forced response. High-frequency variability (e.g., ENSO) is filtered out. \n",
    "\n",
    "#### Outline:\n",
    "\n",
    "* LFCA background\n",
    "* Notes on setup with conda\n",
    "* Import of key packages\n",
    "* User-specified options\n",
    "* Loading in the ForceSMIP data\n",
    "    * Defining a function to read in data\n",
    "    * Loading monthly anomaly maps for the evaluation data\n",
    "    * Calculating the ensemble-mean monthly anomaly maps for the training data\n",
    "* Apply LFCA to the evaluation data for specified hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970b782-8473-4f56-ab76-546f789bc719",
   "metadata": {},
   "source": [
    "### Data structure, training and evaluation data\n",
    "\n",
    "LFCA analyzes a data matrix with a size of the number of timesteps (n<sub>t</sub>) by the number of spatial points (n<sub>lat</sub> x n<sub>lon</sub>). The notebook is set up to use monthly surface air temperature values, though you can change the variable used or try modifying it to use annual means. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=800 src=\"figs/anomaly_maps.png\" />\n",
    "</p>\n",
    "\n",
    "This data matrix will be created for the **training data** and the **evaluation data**. For the training data, we have many ensemble members, and we can compute the forced response as the ensemble average. For the evaluation data, we only have one realization from each model, just as we only have one realization of the real world. The challenge of ForceSMIP is to come up with methods that can approximate the forced response from these single realizations, removing internal variability as we would if we were able to take an ensemble mean.\n",
    "\n",
    "### LFCA\n",
    "\n",
    "The basis for LFCA is principal component analysis (aka EOF analysis). From this we find a set of basis functions (EOFs) that efficiently describe the variability in the data matrix. A key parameter in LFCA is the number of EOFs to include, which is labelled 'trucation' in the code. LFCA then looks for linear combinations of the included EOFs that maximize the ratio of low-frequency to total variance, by comparing the (principal component) amplitude of the patterns in a low-pass filtered version of the data matrix compared to their amplitude in the full data. Here, we use a 10-year (120-month) low-pass filter, but this can be modified. \n",
    "\n",
    "This analysis is schematically depicted below. The result are what we call low-frequency patterns (LFPs) and their low-frequency component (LFC) timeseries. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=800 src=\"figs/LFCA_flowchart.png\" />\n",
    "</p>\n",
    "\n",
    "### Forced response estimate (LFP filtering)\n",
    "\n",
    "To reconstruct an estimate of the forced response, we multiply LFP*LFC for a few of the leading LFP/LFC combinations to once again make a matrix with dimensions of the number of timesteps (n<sub>t</sub>) by the number of spatial points (n<sub>lat</sub> x n<sub>lon</sub>). Wills et al. (2020) calls this LFP filtering. The number of LFP/LFC combinations to include, which is called `N_LFCs` in the code, is the other key hyperparameter of the analysis.\n",
    "\n",
    "### Utilizing the training data (only in ForceSMIP_LFCA.ipynb, skipped for this simple notebook)\n",
    "\n",
    "The training data is used for choosing the hyperparamters `trucation` and `N_LFCs`. Here, the notebook simply tries a few different values of each, then compares the LFP filtered forced reponse estimate to the \"right answer\", which for the case of the training data is the ensemble mean. This notebook shows a few different possible skill estimates, and the user is free to choose which values work best for their needs depending on what they want to optimize, i.e., anomaly correlation coefficient (ACC) vs. root-mean square error (RMSE), spatial pattern of trends vs. spatiotemporal variance at monthly timescales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc296a6e",
   "metadata": {},
   "source": [
    "## Some setup notes\n",
    "\n",
    "If you are using Jupyter (or Python with conda environments), there may be a `forcesmip` kernel/environment pre-installed for you to use. You can also check by clicking `Kernel` > `Change Kernel...` (and then look for a `forcesmip` kernel). You can determine if a conda environment exists by looking for it with `conda info --envs` in a terminal. If you use a pre-installed kernel, you can simply use the pre-installed `forcesmip` kernel and you do not need to continue with this setup.\n",
    "\n",
    "This setup assumes that you have anaconda installed. If you do not, you can install miniconda (from [here](https://docs.conda.io/en/main/miniconda.html)). On some systems you may need to load or activate conda (e.g., via `module load conda` on NCAR systems). Once conda is installed and active, you can create a forcesmip environment with (the `-y` flag create the environment without a confirmation prompt): \n",
    "\n",
    "`conda create -n forcesmip -c conda-forge xcdat xesmf scikit-learn scipy eofs matplotlib cartopy nc-time-axis ipython ipykernel tensorflow python=3.9 -y`\n",
    "\n",
    "> NOTE: This may take a long time (i.e., hours!). If you're in a rush and are not using neural networks, you can remove `tensorflow` from `conda install` and install it afterwards with `pip install tensorflow` (if desired). \n",
    "\n",
    "Activate your environment with: `conda activate forcesmip` or `source activate forcesmip`\n",
    "\n",
    "If you'd like to be able to use this environment with Jupyter, you need to install it with:\n",
    "\n",
    "`python -m ipykernel install --user --name forcesmip --display-name forcesmip`\n",
    "\n",
    "### Import packages (code starts here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395aed7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I/O / data wrangling\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xcdat as xc\n",
    "\n",
    "# data analysis\n",
    "from eofs.standard import Eof\n",
    "from scipy.signal import convolve, butter, filtfilt\n",
    "\n",
    "# runtime metrics\n",
    "import time as clocktime\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "# define a lambda function to perform natural sort\n",
    "natsort = lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(\"(\\d+)\", s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15f979-eec9-4811-8cc6-65cc64894612",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define LFCA, Eigenvalue Analysis, and helper functions\n",
    "\n",
    "Look under the hood. Here are the functions needed for LFCA. \n",
    "\n",
    "Note that this is currently setup to use numpy. There is an option to switch it to xarray, which is more efficient, but it requires fixing a bug in the installed eofs package: https://stackoverflow.com/questions/71740621/eofs-xarray-raising-typeerror-using-a-dataarray-to-construct-a-variable-is-ambi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e34a5e1-262a-42a1-867a-28674688ff6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "New xarray implementation of Low-Frequency Component Analysis (LFCA)\n",
    "\n",
    "Author: Oliver Mehling (University of Torino)\n",
    "Based on Numpy Version by: Zhaoyi Shen (Caltech)\n",
    "Minor edits by: Robb Jnglin Wills (ETH Zurich)\n",
    "\n",
    "Reference:  Wills et al.: Disentangling Global Warming, Multidecadal Variability,\n",
    "            and El Niño in Pacific Temperatures,\n",
    "            Geophysical Research Letters 45, 2487-2496 (2018)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from eofs.standard import Eof\n",
    "from scipy.signal import convolve, butter, filtfilt\n",
    "\n",
    "### Helper functions ###\n",
    "def low_pass_weights(window, cutoff):\n",
    "    \"\"\"Calculate weights for a low pass Lanczos filter.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    window: int\n",
    "        The length of the filter window.\n",
    "\n",
    "    cutoff: float\n",
    "        The cutoff frequency in inverse time steps.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "        from https://scitools.org.uk/iris/docs/v1.2/examples/graphics/SOI_filtering.html\n",
    "\n",
    "    \"\"\"\n",
    "    order = ((window - 1) // 2 ) + 1\n",
    "    nwts = 2 * order + 1\n",
    "    w = np.zeros([nwts])\n",
    "    n = nwts // 2\n",
    "    w[n] = 2 * cutoff\n",
    "    k = np.arange(1., n)\n",
    "    sigma = np.sin(np.pi * k / n) * n / (np.pi * k)\n",
    "    firstfactor = np.sin(2. * np.pi * cutoff * k) / (np.pi * k)\n",
    "    w[n-1:0:-1] = firstfactor * sigma\n",
    "    w[n+1:-1] = firstfactor * sigma\n",
    "    return w[1:-1]\n",
    "\n",
    "def filter_padding(ts, window, ftype='mirror', detrend=True, detrend_poly=1): \n",
    "    ts_pad = np.zeros(2*window+len(ts))\n",
    "    if detrend:\n",
    "        t = np.arange(len(ts))\n",
    "        z = np.polyfit(t,ts,detrend_poly)\n",
    "        p = np.poly1d(z)\n",
    "        ts_in = ts-p(t)\n",
    "    else:\n",
    "        ts_in = ts\n",
    "    ts_pad[window:-window] = ts_in[:]\n",
    "    if ftype == 'mirror':\n",
    "        ts_pad[:window] = ts_in[:window][::-1]\n",
    "        ts_pad[-window:] = ts_in[-window:][::-1]\n",
    "    elif ftype == 'periodic':\n",
    "        ts_pad[:window] = ts_in[-window:]\n",
    "        ts_pad[-window:] = ts_in[:window]\n",
    "    else:\n",
    "        raise ValueError('in filter_padding: ftype must be one of \"mirror\" or \"periodic\".')\n",
    "    if detrend:\n",
    "        t_pad = np.arange(-window,len(ts)+window)\n",
    "        ts_pad = ts_pad+p(t_pad)\n",
    "    return ts_pad\n",
    "\n",
    "def filter_ts(ts, cutoff, filter_type='lanczos', padding_type='mirror', detrend=True, detrend_poly=1): \n",
    "    lanczos_weights=low_pass_weights(cutoff*2+1,1./cutoff) # weights for Lanczos filter\n",
    "    n_pad=int(np.ceil(len(lanczos_weights)/2))\n",
    "    \n",
    "    # Padding\n",
    "    ts_mirr=filter_padding(ts,n_pad,padding_type,detrend=detrend,detrend_poly=detrend_poly)\n",
    "    \n",
    "    # Filtering\n",
    "    if filter_type=='lanczos':\n",
    "        # Lanczos filter\n",
    "        return convolve(ts_mirr,lanczos_weights,'same')[n_pad:-n_pad]\n",
    "    elif filter_type=='butter':\n",
    "        # Butterworth filter\n",
    "        # TODO: here, the cutoff frequency needs to be doubled\n",
    "        # to obtain the same result as for Lanczos - why?\n",
    "        b,a = butter(3,1./(cutoff/2),btype='low')\n",
    "        return filtfilt(b,a,ts_mirr)[n_pad:-n_pad]\n",
    "    else:\n",
    "        raise ValueError('in filter_ts: filter_type must be one of \"lanczos\" or \"butter\".')\n",
    "        \n",
    "### Main LFCA function ###\n",
    "def lfca(x, cutoff, truncation, weights, **kwargs):\n",
    "    if x.ndim!=2:\n",
    "        raise ValueError('x must have dimension 2 for LFCA')\n",
    "    \n",
    "    # Scale vector from weights\n",
    "    scale = np.sqrt(np.transpose(weights)/np.sum(weights))\n",
    "    \n",
    "    # center data\n",
    "    x = x - np.mean(x,axis=0)\n",
    "    xs = x * scale.T\n",
    "    \n",
    "    # Compute EOFs using eofs package\n",
    "    # xarray version: eofs_xr=Eof(xs, center=False, ddof=1)\n",
    "    eofs_xr=Eof(xs.values, center=False, ddof=1)\n",
    "    # Principal component time series (PC_k)\n",
    "    pcs=eofs_xr.pcs(npcs=truncation, pcscaling=1)\n",
    "    \n",
    "    # Filtering of PCs\n",
    "    pcs_filt=np.zeros(pcs.shape)\n",
    "    for i in range(truncation):\n",
    "        pci = pcs[:,i]\n",
    "        pci_filt = filter_ts(pci, cutoff, **kwargs)\n",
    "        pcs_filt[:,i]=pci_filt[:]\n",
    "        #print(np.std(pci),np.std(pci_filt))\n",
    "    \n",
    "    # Compute low-frequency components\n",
    "    cov_lowfreq=np.cov(pcs_filt,rowvar=False)\n",
    "    eig_lowfreq, eigv_lowfreq = np.linalg.eigh(cov_lowfreq) # Eigenvalues r_k, Eigenvectors e_k\n",
    "    eig_argsort = np.argsort(eig_lowfreq)[::-1].copy() # Guarantee that eigenvalues are sorted in descending order\n",
    "    eig_lowfreq = eig_lowfreq[eig_argsort].copy()\n",
    "    eigv_lowfreq = eigv_lowfreq[:,eig_argsort].copy()\n",
    "\n",
    "    uvec=eofs_xr.eofs(neofs=truncation, eofscaling=1).T@eigv_lowfreq # u_k\n",
    "    # xarray version: lfcs=xs@uvec # Low-frequency components (LFC_k)\n",
    "    lfcs = np.dot(xs, uvec)\n",
    "    lfps=eofs_xr.eofs(neofs=truncation, eofscaling=2).T@eigv_lowfreq # v_k = Low-frequency patterns (LFP_k)\n",
    "    # NB: Alternative formula (lfps=xs.T@lfcs) gives the right pattern, but wrong scaling\n",
    "    \n",
    "    # Choose positive sign of LFCs/LFPs\n",
    "    for j in range(lfps.shape[1]):\n",
    "        if np.dot(lfps[:,j], scale.flatten())<0:\n",
    "            lfps[:,j] = -lfps[:,j]\n",
    "            lfcs[:,j] = -lfcs[:,j]\n",
    "    lfps=lfps/scale # Re-scale LFPs (non-weighted)\n",
    "    lfps = np.transpose(lfps)\n",
    "    \n",
    "    return lfcs, lfps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e4284",
   "metadata": {},
   "source": [
    "### Define some static mappings for CMIP/ForceSMIP data\n",
    "\n",
    "This is just some helper information to helps us search for data and reshape it. All ForceSMIP data is on a 2.5 x 2.5 degree lat/lon grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac0ddd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmipTable = {\n",
    "    \"pr\": \"Amon\",\n",
    "    \"psl\": \"Amon\",\n",
    "    \"tas\": \"Amon\",\n",
    "    \"zmta\": \"Amon\",\n",
    "    \"tos\": \"Omon\",\n",
    "    \"siconc\": \"OImon\",\n",
    "    \"monmaxpr\": \"Aday\",\n",
    "    \"monmaxtasmax\": \"Aday\",\n",
    "    \"monmintasmin\": \"Aday\",\n",
    "}\n",
    "cmipVar = {\n",
    "    \"pr\": \"pr\",\n",
    "    \"psl\": \"psl\",\n",
    "    \"tas\": \"tas\",\n",
    "    \"zmta\": \"ta\",\n",
    "    \"tos\": \"tos\",\n",
    "    \"siconc\": \"siconc\",\n",
    "    \"monmaxpr\": \"pr\",\n",
    "    \"monmaxtasmax\": \"tasmax\",\n",
    "    \"monmintasmin\": \"tasmin\",\n",
    "}\n",
    "evalPeriods = {\n",
    "    \"Tier1\": (\"1950-01-01\", \"2022-12-31\"),\n",
    "    \"Tier2\": (\"1900-01-01\", \"2022-12-31\"),\n",
    "    \"Tier3\": (\"1979-01-01\", \"2022-12-31\"),\n",
    "}\n",
    "nlat = 72\n",
    "nlon = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef11685",
   "metadata": {},
   "source": [
    "### Define user-specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0221d736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = \"/net/krypton/climdyn_nobackup/FTP/ForceSMIP/\"  # path to forcesmip data (ETH)\n",
    "#root_dir = \"/glade/campaign/cgd/cas/asphilli/ForceSMIP/\"  # path to forcesmip data (NCAR)\n",
    "\n",
    "outdir = \"data/\"  # directory where output data should be saved\n",
    "\n",
    "ncvar = \"tos\"  # CMIP variable name to be used\n",
    "\n",
    "cutoff = 120 # months (lowpass cutoff) - you can also try varying this parameter\n",
    "truncation = 30 # EOF truncation\n",
    "N_LFC = 2 # how many LFCs to include in the forced response\n",
    "\n",
    "# choose models for training\n",
    "# this version of the LFCA code skips the training. See ForceSMIP_LFCA.ipynb for an idea of how to train the method \n",
    "# (to select the values of truncation and N_LFC that work best)\n",
    "\n",
    "# choose evaluation data\n",
    "eval_tier = \"Tier2\"  # Tier1, Tier2, or Tier3\n",
    "\n",
    "# no need to modify the training or reference period for LFCA\n",
    "tv_time_period = evalPeriods[eval_tier] # (\"1950-01-01\",\"2022-12-30\")  # period of time to consider data for training\n",
    "reference_period = tv_time_period # anomalies will be with respect to mean over entire period (convectional for LFCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de6f47-bf4f-4f59-aec7-44534c057b4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define a function to read in data\n",
    "\n",
    "We're going to loop over many models and realizations for training and evaluation data. To make this more readable and to reduce repeating code, we are going to define a function to do this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e5b82e-7c9b-4643-bfa1-d789e4cc4270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_realization(fn, vid, time_period, reference_period):\n",
    "    \"\"\"\n",
    "    load_realization(fn, vid, time_period, reference_period)\n",
    "    \n",
    "    Function loads in data for a given file, fn, and variable, vid. It\n",
    "    selects data for a given time_period and calculates the anomalies\n",
    "    relative to a user-defined reference_period. The function returns arrays\n",
    "    of the dimensions (time, lat, lon), the 3D anomaly map, and the global\n",
    "    mean time series. \n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    fn (str) : filename\n",
    "    vid (str) : variable id\n",
    "    time_period (tuple(str, str)) : tuple of the start and end of the time period\n",
    "                                    e.g., (\"1900-01-01\", \"1949-12-31\")\n",
    "    reference_period (tuple(str, str)) : tuple of the start and end of the reference period\n",
    "                                         used to calculate anomalies e.g., (\"1900-01-01\", \"1949-12-31\")\n",
    "                                         \n",
    "    Returns:\n",
    "    --------\n",
    "    ts_3d (xr.DataArray) : monthly average anomaly values [time, lat, lon]\n",
    "    ts_gm (xr.DataArray) : monthly average, global mean anomaly values\n",
    "    \"\"\"\n",
    "    # open dataset\n",
    "    ds = xc.open_dataset(fn)\n",
    "    # if specified, subset training/validation data to specific period\n",
    "    if tv_time_period is not None:\n",
    "        ds = ds.sel(time=slice(time_period[0], time_period[1]))\n",
    "    # get departures\n",
    "    ds = ds.temporal.departures(vid, freq=\"month\", reference_period=reference_period)\n",
    "    # If you wanted annual averages instead, you could use the following:\n",
    "    # ds = ds.temporal.group_average(vid, freq=\"year\", weighted=False)\n",
    "    ts_3d = ds[vid]\n",
    "    # take spatial average\n",
    "    ds = ds.spatial.average(vid)\n",
    "    ts_gm = ds[vid]\n",
    "    # clean up \n",
    "    ds.close()\n",
    "    # return values\n",
    "    return ts_3d, ts_gm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00ec62-e110-4242-8c42-129a939fc1df",
   "metadata": {},
   "source": [
    "### Read in evaluation data\n",
    "\n",
    "We start with the evaluation data, because this data includes observations...and missing data. We will load the evaluation data and create a missing data mask, which we will apply to the training/evaluation data as well. That way the training and evaluation data will be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25926baf-3092-4b99-9c68-d5444e4fc5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10: 2A\n",
      "2 / 10: 2B\n",
      "3 / 10: 2C\n",
      "4 / 10: 2D\n",
      "5 / 10: 2E\n",
      "6 / 10: 2F\n",
      "7 / 10: 2G\n",
      "8 / 10: 2H\n",
      "9 / 10: 2I\n",
      "10 / 10: 2J\n"
     ]
    }
   ],
   "source": [
    "# first we search for the evaluation data\n",
    "epath = \"/\".join([root_dir, \"Evaluation-\" + eval_tier, cmipTable[ncvar], ncvar])\n",
    "efiles = glob.glob(epath + \"/*.nc\")\n",
    "efiles = sorted(efiles, key=natsort)\n",
    "\n",
    "# initialize dictionary to store data\n",
    "evaluation_anomaly_maps = {}\n",
    "missing_data_mask = {}\n",
    "vid = cmipVar[ncvar]\n",
    "\n",
    "# loop over evaluation files\n",
    "for im, fn in enumerate(efiles):\n",
    "    # get evaluation identifier\n",
    "    model = fn.split(\"/\")[-1].split(\"_\")[2].split(\".\")[0]\n",
    "    # print progress\n",
    "    print(str(im + 1) + \" / \" + str(len(efiles)) + \": \" + model)\n",
    "    # read in data for realization\n",
    "    ts_3d, ts_gm = load_realization(fn, vid, tv_time_period, reference_period)\n",
    "    # store anomaly map\n",
    "    evaluation_anomaly_maps[model] = ts_3d\n",
    "    \n",
    "    # create mask for missing data\n",
    "    tmp = np.mean(ts_3d, axis=0)\n",
    "    missing_data_mask[model] = np.where(np.isnan(tmp), np.nan, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0c042-de61-4284-b5dc-b1f3c7489176",
   "metadata": {},
   "source": [
    "## LFCA of evaluation members\n",
    "\n",
    "Now that we have undergone the time intensive task of training, we can choose values of 'truncation' and 'N_EOFs' based on the above plots (there is no right way to do this), apply LFCA to the evalution members, and visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3113783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2A\n",
      "Model 2B\n",
      "Model 2C\n",
      "Model 2D\n",
      "Model 2E\n",
      "Model 2F\n",
      "Model 2G\n",
      "Model 2H\n",
      "Model 2I\n",
      "Model 2J\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionaries for LFCA output\n",
    "lfcs_eval_all = {}\n",
    "lfps_eval_all = {}\n",
    "X_forced_eval_all = {}\n",
    "X_total_eval_all = {}\n",
    "\n",
    "eval_models = evaluation_anomaly_maps.keys()\n",
    "\n",
    "for ie, eval_model in enumerate(eval_models):\n",
    "    \n",
    "    print(\"Model \" + eval_model)\n",
    "    \n",
    "    # convert missing data mask from nans to zeros\n",
    "    mask = np.nan_to_num(missing_data_mask[eval_model], nan=0)\n",
    "    \n",
    "    lfcs_eval_all[eval_model] = {}\n",
    "    lfps_eval_all[eval_model] = {}\n",
    "    X_forced_eval_all[eval_model] = {}\n",
    "    X_total_eval_all[eval_model] = {}\n",
    "    \n",
    "    # this is the data from each training model/member\n",
    "    sst_anomalies = evaluation_anomaly_maps[eval_model]\n",
    "\n",
    "    # extract coordinate vectors (same for all models, so could be moved out of the loop)\n",
    "    lon_axis = evaluation_anomaly_maps[eval_model].lon\n",
    "    lat_axis = evaluation_anomaly_maps[eval_model].lat\n",
    "    time = evaluation_anomaly_maps[eval_model].time\n",
    "    # dimensions of the data matrix\n",
    "    n_timesteps = len(time)\n",
    "    nfeatures = nlat * nlon\n",
    "    \n",
    "    # create cos(lat) weighting, with mask for missing data \n",
    "    y, x = np.meshgrid(lat_axis,lon_axis)\n",
    "    area = np.cos(y*np.pi/180.) * np.transpose(mask)\n",
    "    area_weights = np.reshape(area,(nfeatures,1),order='F')\n",
    "\n",
    "    # combine lat & lon into a single spatial dimension\n",
    "    x_all = sst_anomalies.stack(shape=['lat','lon'])\n",
    "\n",
    "    # these give the spatial points (columns) where there are not and are missing data, in icol_ret and icol_disc, respectively\n",
    "    icol_ret = np.where(area_weights!=0)\n",
    "    icol_disc = np.where(area_weights==0)\n",
    "    # remove missing spatial points from x and area_weights\n",
    "    x = x_all[:,icol_ret[0]]\n",
    "    area_weights = np.transpose(area_weights[icol_ret[0],:])\n",
    "\n",
    "    # perform LFCA for each evaluation model \n",
    "    lfcs, lfps =  lfca(x, cutoff, truncation, area_weights)\n",
    "\n",
    "    # put back in missing data \n",
    "    nins = len(icol_disc[0])\n",
    "    nrows = lfps.shape[0]\n",
    "    lfps_aug = np.zeros((nrows,lfps.shape[1]+nins))\n",
    "    lfps_aug[:] = np.nan\n",
    "    lfps_aug[:,icol_ret[0]] = lfps\n",
    "    \n",
    "    # time axis\n",
    "    time_plot = [t.year + t.month/12 for t in time.values]\n",
    "    \n",
    "    # forced response estimate from LFP filtering\n",
    "    X_forced = np.matmul(lfcs[:,0:N_LFC],lfps_aug[0:N_LFC,:])\n",
    "    X_forced = X_forced.reshape(n_timesteps,nlat,nlon)\n",
    "    X_forced = xr.DataArray(X_forced, coords=[time_plot,lat_axis,lon_axis], dims=[\"time\",\"lat\",\"lon\"])\n",
    "    \n",
    "    # put output in library\n",
    "    lfcs_eval_all[eval_model] = lfcs\n",
    "    lfps_eval_all[eval_model] = lfps_aug\n",
    "    X_forced_eval_all[eval_model] = X_forced\n",
    "    X_total_eval_all[eval_model] = sst_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dca38-ac9e-4b73-a814-76692a23daad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot the first four LFPs and LFCs for an evaluation member of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14462627-14ed-463c-bb29-b70eae644d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m clev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m1.3\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#clev = np.arange(-200, 205, 5)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m lfcs \u001b[38;5;241m=\u001b[39m \u001b[43mlfcs_eval_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplot_model\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m lfps_aug \u001b[38;5;241m=\u001b[39m lfps_eval_all[plot_model]\n\u001b[1;32m      9\u001b[0m LFCs_plot \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '1A'"
     ]
    }
   ],
   "source": [
    "plot_model = '1A'\n",
    "\n",
    "clev = np.arange(-1.2, 1.3, 0.1)\n",
    "#clev = np.arange(-200, 205, 5)\n",
    "\n",
    "lfcs = lfcs_eval_all[plot_model]\n",
    "lfps_aug = lfps_eval_all[plot_model]\n",
    "\n",
    "LFCs_plot = (1,2,3,4)\n",
    "\n",
    "f=plt.figure(figsize=(11, 16))\n",
    "for i in range(0,4):\n",
    "    pattern = np.reshape(lfps_aug[i,...],(nlon,nlat),order='F')\n",
    "    pattern[np.where(np.abs(pattern)>1.e5)] = np.nan\n",
    "    if i>1:\n",
    "        plt.subplot(4, 2, i+3, projection=ccrs.Robinson())\n",
    "    else:\n",
    "        plt.subplot(4, 2, i+1, projection=ccrs.Robinson())\n",
    "    map_plot = np.transpose(pattern)\n",
    "    cyclic_data, cyclic_longitude = add_cyclic_point(map_plot, coord=lon_axis)\n",
    "    im = plt.contourf(cyclic_longitude, lat_axis, cyclic_data, clev, transform=ccrs.PlateCarree(), cmap=plt.cm.RdBu_r, extend='both')\n",
    "    # coastlines\n",
    "    plt.gca().set_global()\n",
    "    plt.gca().coastlines(alpha=0.2)\n",
    "    # colorbar\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title('LFP '+str(i+1))\n",
    "    if i>1:\n",
    "        plt.subplot(4, 2, i+5)\n",
    "    else:\n",
    "        plt.subplot(4, 2, i+3)\n",
    "    plt.plot(time_plot,lfcs[:,i])\n",
    "    plt.xlabel(\"Year\")\n",
    "    \n",
    "plt.suptitle('LFCs 1-4 for Model '+plot_model)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075cfe5-2df8-4efe-ad82-9f0a73c7b380",
   "metadata": {},
   "source": [
    "## LFP filtered trends over the full time period (based on chosen tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33575fe4-377e-49a8-8d26-72ca77b360cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clevs = np.arange(-2, 2.2, 0.2)\n",
    "#clevs = np.arange(-400, 405, 5)\n",
    "\n",
    "f = plt.figure(figsize=(11, 16))\n",
    "for ie, eval_model in enumerate(eval_models):\n",
    "    plt.subplot(5, 2, ie+1, projection=ccrs.Robinson())\n",
    "    X_forced = X_forced_eval_all[eval_model]\n",
    "    X_forced_trend = X_forced.polyfit('time',1).isel(degree=0).polyfit_coefficients\n",
    "    map_plot = X_forced_trend.values*(time_plot[-1]-time_plot[0])\n",
    "    cyclic_data, cyclic_longitude = add_cyclic_point(map_plot, coord=lon_axis)\n",
    "    im = plt.contourf(cyclic_longitude, lat_axis, cyclic_data, clevs, transform=ccrs.PlateCarree(), cmap=plt.cm.RdBu_r, extend='both')\n",
    "    # coastlines\n",
    "    plt.gca().set_global()\n",
    "    plt.gca().coastlines(alpha=0.2)\n",
    "    plt.title(eval_model)\n",
    "plt.suptitle('LFP Filtered Estimate of Forced Trend')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c19b66-95f4-4045-8edb-63a08909b963",
   "metadata": {},
   "source": [
    "## Visualizing LFP Filtering for Global Mean\n",
    "\n",
    "The real benefits of LFCA aren't necessarily for the trend pattern over the full time period but in getting information about how this forced response pattern evolves in time (i.e., the full spatiotemporal complexity of the forced response). We visualize what LFP filtering is doing to the raw data for the global mean and a few other specific timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afdf60-10e0-4e21-9006-13aff0d24e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_plot = [t.year + t.month/12 for t in time.values]\n",
    "cosw = np.sqrt(np.cos(lat_axis*np.pi/180))\n",
    "f = plt.figure(figsize=(11, 16))\n",
    "for ie, eval_model in enumerate(eval_models):\n",
    "    plt.subplot(5, 2, ie+1)\n",
    "    X_forced = X_forced_eval_all[eval_model]\n",
    "    X_total = X_total_eval_all[eval_model]\n",
    "    GMST_total = (X_total*cosw).mean('lon').mean('lat')\n",
    "    GMST_forced = (X_forced*cosw).mean('lon').mean('lat')\n",
    "    plt.plot(time_plot,GMST_total)\n",
    "    plt.plot(time_plot,GMST_forced)\n",
    "    plt.title(eval_model+' Global Mean')\n",
    "    plt.legend(['Raw','LFP Filtered'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f95d3-0ff8-4961-86aa-f93f08f29bad",
   "metadata": {},
   "source": [
    "## And the same visualization for Niño3.4 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaba901-f722-4e8b-aff5-3b22d52ee350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_plot = [t.year + t.month/12 for t in time.values]\n",
    "cosw = np.sqrt(np.cos(lat_axis*np.pi/180))\n",
    "f = plt.figure(figsize=(11, 16))\n",
    "for ie, eval_model in enumerate(eval_models):\n",
    "    plt.subplot(5, 2, ie+1)\n",
    "    X_forced = X_forced_eval_all[eval_model]\n",
    "    X_total = X_total_eval_all[eval_model]\n",
    "    Nino34_forced = (X_forced*cosw).sel(lon=slice(190,240),lat=slice(-5,5)).mean('lon').mean('lat')\n",
    "    Nino34_total = (X_total*cosw).sel(lon=slice(190,240),lat=slice(-5,5)).mean('lon').mean('lat')\n",
    "    # if you want to compute the global mean and subtract that from Nino34:\n",
    "    #GMST_total = (X_total*cosw).mean('lon').mean('lat')\n",
    "    #GMST_forced = (X_forced*cosw).mean('lon').mean('lat')\n",
    "    plt.plot(time_plot,Nino34_total)\n",
    "    plt.plot(time_plot,Nino34_forced)\n",
    "    plt.title(eval_model+' Niño3.4')\n",
    "    plt.legend(['Raw','LFP Filtered'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bc71a-4d22-4d86-aa7f-666e89cee8c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## And the same visualization for the grid cell corresponding to Boulder or Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce09f2d-ba96-46b3-abd2-95bbbbb52c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = 'Zurich'\n",
    "\n",
    "if ncvar == \"tos\":\n",
    "    print('Skipping plot for ocean variable')\n",
    "else:\n",
    "    if location == 'Zurich':\n",
    "        sel_lat = 47.4 # closest to 47.4°N\n",
    "        sel_lon = 8.5 # closest to 8.5°E\n",
    "    elif ftype == 'Boulder':\n",
    "        sel_lat = 40.02 # closest to 40.02°N\n",
    "        sel_lon = 254.7 #closest to 105.3°W\n",
    "    else:\n",
    "        raise ValueError('Only has options for Zurich and Boulder.')\n",
    "\n",
    "    time_plot = [t.year + t.month/12 for t in time.values]\n",
    "    cosw = np.sqrt(np.cos(lat_axis*np.pi/180))\n",
    "    f = plt.figure(figsize=(11, 16))\n",
    "    for ie, eval_model in enumerate(eval_models):\n",
    "        plt.subplot(5, 2, ie+1)\n",
    "        X_forced = X_forced_eval_all[eval_model]\n",
    "        X_total = X_total_eval_all[eval_model]\n",
    "        Nino34_forced = (X_forced).sel(lon=8.75, lat=47.4, method='nearest')\n",
    "        Nino34_total = (X_total).sel(lon=8.75, lat=47.4, method='nearest')\n",
    "        # if you want to compute the global mean and subtract that from Nino34:\n",
    "        #GMST_total = (X_total*cosw).mean('lon').mean('lat')\n",
    "        #GMST_forced = (X_forced*cosw).mean('lon').mean('lat')\n",
    "        plt.plot(time_plot,Nino34_total)\n",
    "        plt.plot(time_plot,Nino34_forced)\n",
    "        plt.title(eval_model+' '+location)\n",
    "        plt.legend(['Raw','LFP Filtered'])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778a1ef",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "\n",
    "* How would you choose between optimizing the skill metrics presented? Can you think of other skill metrics to use?\n",
    "* We've evaluated the monthly variability and the full-period trend. How about timescales in between (e.g., annual means, 5-year means)? \n",
    "* Do you think this approach (LFCA) would work well for other variables of interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd10f0-e90b-4994-bd50-9e9c7b7a8770",
   "metadata": {},
   "source": [
    "### NetCDF Output of Forced Response Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9d117-f1cf-4802-b801-a7a47d481cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = \"1H\"\n",
    "name = \"LFCA.Wills\"\n",
    "\n",
    "# ensure directory exists to write out data\n",
    "if not os.path.exists('data/'):\n",
    "    os.mkdir(outdir)\n",
    "    \n",
    "filename = outdir+eval_model+\".\"+ncvar+\".\"+name+\".nc\"\n",
    "\n",
    "X_forced = X_forced_eval_all[eval_model]\n",
    "X_forced_trend = X_forced.polyfit('time',1).isel(degree=0).polyfit_coefficients*np.ceil(time_plot[-1]-time_plot[0])\n",
    "X_forced_trend = X_forced_trend.rename(\"forced_component\")\n",
    "\n",
    "print ('saving to ', filename)\n",
    "X_forced_trend.to_netcdf(path=filename)\n",
    "X_forced_trend.close()\n",
    "print ('finished saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45b11c-3a66-4f85-b71e-f8ab0324bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Teaching - forcesmip",
   "language": "python",
   "name": "teach_forcesmip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
